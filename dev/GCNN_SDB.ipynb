{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#GCNN-SDB (Gated Convolutional Neural Network with Simulated Dendritic Branches)"
      ],
      "metadata": {
        "id": "-9YDYZZHTZvu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Derived from https://github.com/DavidWBressler/GCNN/blob/master/GCNN.ipynb"
      ],
      "metadata": {
        "id": "hZrqt4LjTUwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "useSDB = True\n",
        "if(useSDB):\n",
        "    numberOfIndependentDendriticBranches = 10   #4\n",
        "    normaliseActivationSparsity = True\n",
        "\n",
        "\t\t\t"
      ],
      "metadata": {
        "id": "2hONjWuwDJLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UUlC9OqYWrt"
      },
      "source": [
        "# Download and preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy==2.2.4"
      ],
      "metadata": {
        "id": "1hmH1GpUhwIN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dab7a81-a48d-45b7-dcb7-8de7971c45c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spacy==2.2.4\n",
            "  Downloading spacy-2.2.4-cp38-cp38-manylinux1_x86_64.whl (10.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting plac<1.2.0,>=0.9.6\n",
            "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy==2.2.4) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy==2.2.4) (2.25.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy==2.2.4) (4.64.1)\n",
            "Collecting blis<0.5.0,>=0.4.0\n",
            "  Downloading blis-0.4.1-cp38-cp38-manylinux1_x86_64.whl (3.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy==2.2.4) (0.10.1)\n",
            "Collecting srsly<1.1.0,>=1.0.2\n",
            "  Downloading srsly-1.0.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 KB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting catalogue<1.1.0,>=0.0.7\n",
            "  Downloading catalogue-1.0.2-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy==2.2.4) (1.21.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy==2.2.4) (2.0.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy==2.2.4) (1.0.9)\n",
            "Collecting thinc==7.4.0\n",
            "  Downloading thinc-7.4.0-cp38-cp38-manylinux1_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy==2.2.4) (3.0.8)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4) (2022.12.7)\n",
            "Installing collected packages: plac, srsly, catalogue, blis, thinc, spacy\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 2.4.5\n",
            "    Uninstalling srsly-2.4.5:\n",
            "      Successfully uninstalled srsly-2.4.5\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 2.0.8\n",
            "    Uninstalling catalogue-2.0.8:\n",
            "      Successfully uninstalled catalogue-2.0.8\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.7.9\n",
            "    Uninstalling blis-0.7.9:\n",
            "      Successfully uninstalled blis-0.7.9\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.6\n",
            "    Uninstalling thinc-8.1.6:\n",
            "      Successfully uninstalled thinc-8.1.6\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.4\n",
            "    Uninstalling spacy-3.4.4:\n",
            "      Successfully uninstalled spacy-3.4.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.4.1 requires spacy<3.5.0,>=3.4.0, but you have spacy 2.2.4 which is incompatible.\n",
            "confection 0.0.3 requires srsly<3.0.0,>=2.4.0, but you have srsly 1.0.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed blis-0.4.1 catalogue-1.0.2 plac-1.1.3 spacy-2.2.4 srsly-1.0.6 thinc-7.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastai==1.0.61    #1.6.1\n",
        "#https://github.com/fastai/fastai1"
      ],
      "metadata": {
        "id": "Fit7pYx5dMV2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd4b666f-d00d-4f15-bd64-5b34a54f22f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fastai==1.0.61\n",
            "  Downloading fastai-1.0.61-py3-none-any.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.2/239.2 KB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.8/dist-packages (from fastai==1.0.61) (1.21.6)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.8/dist-packages (from fastai==1.0.61) (2.8.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from fastai==1.0.61) (2.25.1)\n",
            "Collecting nvidia-ml-py3\n",
            "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from fastai==1.0.61) (1.7.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from fastai==1.0.61) (21.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from fastai==1.0.61) (7.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from fastai==1.0.61) (6.0)\n",
            "Requirement already satisfied: fastprogress>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from fastai==1.0.61) (1.0.3)\n",
            "Collecting bottleneck\n",
            "  Downloading Bottleneck-1.3.5-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (355 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m355.2/355.2 KB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from fastai==1.0.61) (1.13.0+cu116)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from fastai==1.0.61) (3.2.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from fastai==1.0.61) (4.6.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from fastai==1.0.61) (1.3.5)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from fastai==1.0.61) (0.14.0+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.0.0->fastai==1.0.61) (4.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->fastai==1.0.61) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->fastai==1.0.61) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->fastai==1.0.61) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->fastai==1.0.61) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->fastai==1.0.61) (2022.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->fastai==1.0.61) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->fastai==1.0.61) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->fastai==1.0.61) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->fastai==1.0.61) (4.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->fastai==1.0.61) (1.15.0)\n",
            "Building wheels for collected packages: nvidia-ml-py3\n",
            "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19190 sha256=0aa964dbffe0ef48096e94681b60fb6dd1b71c80683295aa153796ef66f9b5b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/b1/68/cb4feab29709d4155310d29a421389665dcab9eb3b679b527b\n",
            "Successfully built nvidia-ml-py3\n",
            "Installing collected packages: nvidia-ml-py3, bottleneck, fastai\n",
            "  Attempting uninstall: fastai\n",
            "    Found existing installation: fastai 2.7.10\n",
            "    Uninstalling fastai-2.7.10:\n",
            "      Successfully uninstalled fastai-2.7.10\n",
            "Successfully installed bottleneck-1.3.5 fastai-1.0.61 nvidia-ml-py3-7.352.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4UyfHt5YWr3"
      },
      "outputs": [],
      "source": [
        "from fastai import *\n",
        "from fastai.text import * \n",
        "from fastai.imports import *\n",
        "\n",
        "import torch.utils.data as data_utils\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data.sampler import Sampler\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import time\n",
        "import importlib\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udmkUFJNYWr6"
      },
      "outputs": [],
      "source": [
        "DATAPATH = Path('data/GCNN/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-y7nHqCYWr-"
      },
      "outputs": [],
      "source": [
        "sns.set() #set graph formatting to seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmRxdXZ1YWr_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65e71aae-3277-46d0-8aa4-afa48005cfcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-09 23:37:28--  https://s3.amazonaws.com/fast-ai-nlp/wikitext-2.tgz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.60.192, 52.217.89.238, 52.217.132.136, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.60.192|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4070055 (3.9M) [application/x-tar]\n",
            "Saving to: ‘data/wikitext-2.tgz’\n",
            "\n",
            "wikitext-2.tgz      100%[===================>]   3.88M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-01-09 23:37:28 (28.1 MB/s) - ‘data/wikitext-2.tgz’ saved [4070055/4070055]\n",
            "\n",
            "--2023-01-09 23:37:28--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2023-01-09 23:37:29--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-01-09 23:37:29--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘data/GCNN/glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.10MB/s    in 2m 39s  \n",
            "\n",
            "2023-01-09 23:40:08 (5.19 MB/s) - ‘data/GCNN/glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  data/GCNN/glove.6B.zip\n",
            "  inflating: data/GCNN/glove.6B.50d.txt  \n",
            "  inflating: data/GCNN/glove.6B.100d.txt  \n",
            "  inflating: data/GCNN/glove.6B.200d.txt  \n",
            "  inflating: data/GCNN/glove.6B.300d.txt  \n"
          ]
        }
      ],
      "source": [
        "#download wikitext-2 dataset and GloVe embeddings\n",
        "!wget https://s3.amazonaws.com/fast-ai-nlp/wikitext-2.tgz -P data\n",
        "!tar xzf data/wikitext-2.tgz -C data\n",
        "!mv data/wikitext-2/ data/GCNN/\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip -P data/GCNN/\n",
        "!unzip data/GCNN/glove.6B.zip -d data/GCNN/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CT2-JpN3YWsD"
      },
      "outputs": [],
      "source": [
        "#Do some preprocessing of the data:\n",
        "\n",
        "#put data into df's w/ columns for 'labels' and 'text'\n",
        "df_trn = pd.read_csv(DATAPATH/'train.csv',header=None,names=['text'])\n",
        "df_test = pd.read_csv(DATAPATH/'test.csv',header=None,names=['text'])\n",
        "df_trn['labels']=0\n",
        "df_test['labels']=0\n",
        "df_trn=df_trn[['labels','text']]\n",
        "df_test=df_test[['labels','text']]\n",
        "\n",
        "#split data into paragraphs, then remove paragraphs <10 or >300 words\n",
        "trn_paragraphs=[]\n",
        "for docnum in range(len(df_trn)):\n",
        "    trn_paragraphs.extend([x for x in df_trn.iloc[docnum].text.split('\\n')])\n",
        "trn_paragraphs.sort(key=len)\n",
        "trn_paragraphs=[par for par in trn_paragraphs if (len(par.split(' '))<300 and len(par.split(' '))>10)]#remove paragraphs >300 and <10 words\n",
        "trn_paragraphs=[par+'xxeos ' for par in trn_paragraphs] #add EOS token at end of each paragraph\n",
        "\n",
        "test_paragraphs=[]\n",
        "for docnum in range(len(df_test)):\n",
        "    test_paragraphs.extend([x for x in df_test.iloc[docnum].text.split('\\n')])\n",
        "test_paragraphs.sort(key=len)\n",
        "test_paragraphs=[par for par in test_paragraphs if (len(par.split(' '))<300 and len(par.split(' '))>10)]#remove paragraphs >300 and <10 words\n",
        "test_paragraphs=[par+'xxeos ' for par in test_paragraphs] #add EOS token at end of each paragraph\n",
        "\n",
        "#put data into csv's\n",
        "df_trn_par = pd.DataFrame({'text':trn_paragraphs})\n",
        "df_test_par = pd.DataFrame({'text':test_paragraphs})\n",
        "\n",
        "df_trn_par['labels']=0\n",
        "df_test_par['labels']=0\n",
        "df_trn_par=df_trn_par[['labels','text']]\n",
        "df_test_par=df_test_par[['labels','text']]\n",
        "\n",
        "df_trn_par.to_csv(DATAPATH/'train_proc_par2.csv', header=False, index=False)\n",
        "df_test_par.to_csv(DATAPATH/'test_proc_par2.csv', header=False, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D69gdmeNYWsG"
      },
      "source": [
        "# Create modeler class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcRV38iCYWsK"
      },
      "outputs": [],
      "source": [
        "class modeler():\n",
        "    def __init__(self,trn_dl,val_dl,module,modelvals=None):\n",
        "        self.trn_dl, self.val_dl, self.module = trn_dl, val_dl, module\n",
        "        self.modelvals=modelvals\n",
        "        self.model=self.module.cuda()\n",
        "    def model_fit(self):\n",
        "        samp_n=self.modelvals['samp_n']#the number of iterations in an epoch\n",
        "        starttime=time.time()\n",
        "        train_loss_list=[]; val_loss_list=[]\n",
        "        for epoch in range(0, self.modelvals['epochs']):\n",
        "            pbar=0#progressbar\n",
        "            for batch_idx, (data, target) in enumerate(self.trn_dl):\n",
        "                \n",
        "                #GRAB MINIBATCH OF INPUTS AND TARGETS, SET OPTIMIZER\n",
        "                data = Variable(data)\n",
        "                pbar+=self.modelvals['bs'] #how many iterations have we done in the epoch so far\n",
        "                if self.modelvals['opttype']=='sgd':\n",
        "                    self.optimizer = optim.SGD(self.model.parameters(), lr=self.modelvals['lr'], \n",
        "                                       momentum=self.modelvals['mom'], weight_decay=self.modelvals['wd'],\n",
        "                                              nesterov=self.modelvals['nesterov'])\n",
        "                elif self.modelvals['opttype']=='adam':\n",
        "                    self.optimizer = optim.Adam(self.model.parameters(), lr=self.modelvals['lr'], \n",
        "                                        betas=(self.modelvals['mom'], 0.999))\n",
        "                self.optimizer.zero_grad()\n",
        "                \n",
        "                #FORWARD PASS\n",
        "                output = self.model(data)\n",
        "                \n",
        "                #CALCULATE AND BACKPROP THE LOSS\n",
        "                loss= output.loss\n",
        "                loss.backward()\n",
        "                if self.modelvals['grad_clip']!=0: #gradient clipping\n",
        "                    torch.nn.utils.clip_grad_value_(self.model.parameters(), self.modelvals['grad_clip'])\n",
        "                    \n",
        "                #UPDATE THE WEIGHTS\n",
        "                self.optimizer.step()\n",
        "                \n",
        "                #PRINT OUT TRAINING UPDATES\n",
        "                train_loss_list.append([epoch,pbar+epoch*samp_n,loss.data.item(),self.modelvals['lr']])\n",
        "                if batch_idx % 100 == 0:\n",
        "                    elapsed_time=time.time()-starttime\n",
        "                    train_update_format_string = 'Train Epoch: {}'\n",
        "                    train_update_format_string += '\\tTotal_its: {:.2f}M [{:.2f}M/{:.2f}M]'\n",
        "                    train_update_format_string += '\\tPercdone: {:.2f}'\n",
        "                    train_update_format_string += '\\tLoss: {:.4f}'\n",
        "                    train_update_format_string += '\\tTime: {:.2f}'\n",
        "                    train_update_format_string += '\\tLR: {:.4f}'\n",
        "                    train_update_string=train_update_format_string.format(\n",
        "                            epoch,\n",
        "                            (pbar + epoch * samp_n) / 1000000, pbar / 1000000, samp_n / 1000000,\n",
        "                            pbar / samp_n,\n",
        "                            loss.data.item(),\n",
        "                            elapsed_time / 60,\n",
        "                            self.modelvals['lr'])\n",
        "                    print(train_update_string)\n",
        "            final_train_loss=loss.data.item()\n",
        "            \n",
        "            #NOW TEST VALIDATION SET\n",
        "            val_loss=[]\n",
        "            self.model.eval() #important to set to eval mode for testing, so that eg batchnorm and dropout aren't used\n",
        "            for batch_idx, (data, target) in enumerate(self.val_dl):\n",
        "                data = Variable(data)\n",
        "                self.optimizer.zero_grad()\n",
        "                #ONLY NEED FORWARD PASS... NO BACKPROP\n",
        "                output = self.model(data)\n",
        "                loss= output.loss\n",
        "                output=output.output\n",
        "                val_loss.append(loss.data.item())\n",
        "            self.model.train() #set back to training mode\n",
        "            ave_val_loss=sum(val_loss) / len(val_loss)\n",
        "            val_update_string='Validation Loss: {:.4f}\\tPerp: {:.4f}'.format(\n",
        "                ave_val_loss,np.exp(ave_val_loss))\n",
        "            print(val_update_string)\n",
        "            val_loss_list.append([epoch,ave_val_loss, np.exp(ave_val_loss),elapsed_time/60])\n",
        "        self.modelvals['val_loss_list']=val_loss_list\n",
        "        self.modelvals['train_loss_list']=train_loss_list\n",
        "        print('The end! {:.2f} minutes'.format((time.time()-starttime)/60))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load GCNN_textfuncs.py"
      ],
      "metadata": {
        "id": "VsW79Jx0nAV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#create adaptive-softmax language-model dataset\n",
        "class LMDataset_GCNN(Dataset):\n",
        "    def __init__(self, tokens):\n",
        "        self.tokens=tokens\n",
        "    def __getitem__(self,index):\n",
        "        #token_list=torch.FloatTensor(self.tokens[index]).cuda()\n",
        "        token_list=torch.LongTensor(self.tokens[index])\n",
        "        label=torch.FloatTensor([1])\n",
        "        #label=torch.ones(len(token_list)-1).float()\n",
        "        return token_list,label\n",
        "    def __len__(self):\n",
        "        return len(self.tokens)\n",
        "    \n",
        "    \n",
        "class SortSampler_GCNN(Sampler): #inspired by fast.ai sortsampler... pass in something like key=lambda x: len(val_clas[x])\n",
        "    def __init__(self, data_source, key): self.data_source,self.key = data_source,key\n",
        "    def __len__(self): return len(self.data_source)\n",
        "    def __iter__(self):\n",
        "        return iter(sorted(range(len(self.data_source)), key=self.key, reverse=True))#return iterator in reverse order, sorted by input key (e.g. length)\n",
        "    \n",
        "#this sortishsampler does the following:\n",
        "    # 1) get a list of randomized indices of length of entire dataset\n",
        "    # 2) break that into a list of sublists, each sublist of size bs*50\n",
        "    # 3) create a new list that is sorted within each of those chunks\n",
        "    # 4) break that sorted list into chunks of size bs\n",
        "    # 5) create new list by randomizing order of all those bs-chunks (with bs-chunk w/ largest key first)\n",
        "#this will give batches sorted by key (e.g. length) within the batch\n",
        "class SortishSampler_GCNN(Sampler): #inspired by fast.ai sortishsampler... pass in something like key=lambda x: len(val_clas[x])\n",
        "    def __init__(self, data_length, key,bs): self.data_length,self.key,self.bs = data_length,key,bs\n",
        "    def __len__(self): return self.data_length\n",
        "    def __iter__(self):\n",
        "        idxs = np.random.permutation(self.data_length)#random permutation of length of entire dataset\n",
        "        sz = self.bs*50 #chunk size is bs*50\n",
        "        #range(0, len(idxs), sz) : go through length of entire dataset, with stepsize=chunk_size\n",
        "        #idxs[i:i+sz] :within that chunk's range, get all the indices of the random permutation above\n",
        "        #this creates a list of lists... basically just splitting up idxs into a bunch of chunks\n",
        "        ck_idx = [idxs[i:i+sz] for i in range(0, len(idxs), sz)]\n",
        "        #for s in ck_idx: go through each sublist in the big list\n",
        "        #sorted(s, key=self.key, reverse=True): sort the sublist in reverse order according to the key (e.g. length)\n",
        "        #np.concatenate: concatenate all the sorted chunk sublists together\n",
        "        sort_idx = np.concatenate([sorted(s, key=self.key, reverse=True) for s in ck_idx])\n",
        "        sz = self.bs #now set size to bs\n",
        "        #similar as before, this creates a list of lists, splitting up sort_idx into chunks of size bs\n",
        "        ck_idx = [sort_idx[i:i+sz] for i in range(0, len(sort_idx), sz)]\n",
        "        # go through each bs-chunk, get the key of the first entry (which should be the largest of the chunk)...\n",
        "        # then do argmax to find the chunk with the largest key\n",
        "        max_ck = np.argmax([self.key(ck[0]) for ck in ck_idx])  \n",
        "        #switch spots bw the first chunk and the chunk w/ the max key\n",
        "        ck_idx[0],ck_idx[max_ck] = ck_idx[max_ck],ck_idx[0]\n",
        "        #now randomize the order of all the bs-chunks (except the first), then concatenate together\n",
        "        sort_idx = np.concatenate(np.random.permutation(ck_idx[1:]))\n",
        "        sort_idx = np.concatenate((ck_idx[0], sort_idx))# concatenate the first (largest key val) chunk to the rest\n",
        "        return iter(sort_idx)\n",
        "\n",
        "#inspired by fast.ai's pad_collate\n",
        "def pad_collate_GCNN(samples, pad_idx=1):\n",
        "    #go through all the sentences (found in s[0]), find length of longest\n",
        "    max_len = max([len(s[0]) for s in samples]) \n",
        "    #create a tensor of size [max_len,n_samples], and set all values to pad_idx\n",
        "    res = torch.zeros(max_len, len(samples)).long() + pad_idx \n",
        "    #for each line in res, set so corresponding sentence is aligned to the left edge\n",
        "        #(right-padded: keep padding on the right edge)\n",
        "    for i,s in enumerate(samples): res[:len(s[0]),i] = LongTensor(s[0]) #right-padded\n",
        "    #return res as the padded tensor, and another tensor composed of the labels (found in s[1])\n",
        "\n",
        "\n",
        "    labelsList = [s[1] for s in samples]    #creates a numpy array of (pytorch?) tensor objects\n",
        "    labels = torch.stack(labelsList, dim=0)\n",
        "    labels = torch.FloatTensor(labels)\n",
        "    labels = labels.squeeze().cuda()\n",
        "    return res.cuda(), labels"
      ],
      "metadata": {
        "id": "E8MOX7mpZrL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr24fWUqYWsN"
      },
      "source": [
        "# Set hyperparameters and build embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CqRBVzbGm8LI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KR9m0HEHYWsP"
      },
      "outputs": [],
      "source": [
        "#set hyperparameters\n",
        "bs=50 #batch-size\n",
        "emb_sz=300 #size of the embedding matrix\n",
        "nl=4 #number of layers\n",
        "nh=600 #number hidden units\n",
        "lr=1 #learning rate\n",
        "mom=.95 #momentum\n",
        "wd=5e-5 #weight-decay. Only has effect if opttype==sgd\n",
        "epochs=50\n",
        "nesterov=True #Nesterov momentum. only has effect if opttype==sgd\n",
        "grad_clip=0.07 #gradient clipping value. Set to 0 for no effect. See nn.utils.clip_grad_value_\n",
        "opttype='sgd' #adam, sgd\n",
        "k=4 #kernel_width\n",
        "downbot=20# in the bottleneck layers, how much to decrease channel depth?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QomTfkejYWsR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "c6e9083b-54cc-4588-a088-de1add089f83"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/fastai/core.py:302: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  return np.array(a, dtype=dtype, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Use fast.ai to create a TextLMDataBunch object. See http://docs.fast.ai/text.data.html#class-textlmdatabunch\n",
        "#This tokenizes and numericalizes the data\n",
        "data_lm = TextLMDataBunch.from_csv(path=DATAPATH, csv_name='train_proc_par2.csv', test='test_proc_par2.csv')\n",
        "itos=data_lm.train_ds.vocab.itos# the vocab\n",
        "vs=len(itos)# vs is the length of the vocab\n",
        "\n",
        "#Grab the numericalized data from the TextLMDataBunch dataset, then construct new custom dataset using LMDataset_GCNN\n",
        "trn_tokens=[data_lm.train_ds[i][0].data for i in range(len(data_lm.train_ds))]\n",
        "traindataset=LMDataset_GCNN(trn_tokens)\n",
        "valid_tokens=[data_lm.valid_ds[i][0].data for i in range(len(data_lm.valid_ds))]\n",
        "validdataset=LMDataset_GCNN(valid_tokens)\n",
        "\n",
        "#Create data loaders for training and validation sets\n",
        "trn_samp=SortishSampler_GCNN(data_length=len(traindataset),key=lambda x:len(traindataset[x][0]), bs=bs)\n",
        "val_samp=SortSampler_GCNN(validdataset,key=lambda x:len(validdataset[x][0]))\n",
        "train_loader = data_utils.DataLoader(traindataset, batch_size=bs, collate_fn=pad_collate_GCNN, pin_memory=False, sampler=trn_samp)\n",
        "val_loader = data_utils.DataLoader(validdataset, batch_size=bs, collate_fn=pad_collate_GCNN, pin_memory=False, sampler=val_samp)\n",
        "samp_n=len(traindataset)\n",
        "val_samp_n=len(validdataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80aaC4zIYWsT"
      },
      "outputs": [],
      "source": [
        "#put hyperparameters into a dictionary\n",
        "def get_modelvals():\n",
        "    modelvals=dict((name,eval(name)) for name in [\n",
        "        'lr','mom','wd','opttype','epochs','samp_n','val_samp_n',\n",
        "        'bs','emb_sz','vs', 'nh', 'nl','DATAPATH','nesterov','grad_clip',\n",
        "        'k','downbot'] )\n",
        "    return modelvals\n",
        "\n",
        "modelvals=get_modelvals()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koJWKE_cYWsU"
      },
      "outputs": [],
      "source": [
        "#grab GloVe embeddings:\n",
        "#create vocab itos2 from downloaded glove file\n",
        "words = []\n",
        "idx = 0\n",
        "word2idx = {}\n",
        "vectors = []\n",
        "with open('data/GCNN/glove.6B.300d.txt', 'rb') as f:\n",
        "    for l in f:\n",
        "        line = l.decode().split()\n",
        "        word = line[0]\n",
        "        words.append(word)\n",
        "        word2idx[word] = idx\n",
        "        idx += 1\n",
        "        vectors.append(line[1:])\n",
        "itos2=words\n",
        "\n",
        "#grab the glove embeddings we need, based on the words in our vocab\n",
        "stoi2 = collections.defaultdict(lambda:-1, {v:k for k,v in enumerate(itos2)}) #default -1 means its not in glove's itos2\n",
        "row_m = vectors[-1] #this is default vector... for <unk>\n",
        "new_w = np.zeros((vs, emb_sz), dtype=np.float32)#initialize new weights to zeros of size (vocab_size,embedding size) e.g. (60002,300)... we're creating an embedding matrix \n",
        "for i,w in enumerate(itos): #for index,word in our itos dict, get r index of the word in word2vec's dict. r will be -1 if it doesn't exist in word2vec's dict\n",
        "    r = stoi2[w]#r index of the word in word2vec's dict\n",
        "    new_w[i] = vectors[r] if r>=0 else row_m #for our new embedding matrix, set the embedding at the index from our dict equal to the embedding from index r from word2vec's dict\n",
        "np.save(DATAPATH/'emb_wgts300_proc_par2.npy', new_w) #save the embedding weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5OLlTTBYWsW"
      },
      "source": [
        "# Run GCNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fm3QdhpYYWsX"
      },
      "outputs": [],
      "source": [
        "class GLUblock(nn.Module):\n",
        "    def __init__(self, k, in_c, out_c, downbot):\n",
        "        super().__init__()\n",
        "        #only need to change shape of the residual if num_channels changes (i.e. in_c != out_c)\n",
        "        #[bs,in_c,seq_length]->conv(1,in_c,out_c)->[bs,out_c,seq_length]\n",
        "        if in_c == out_c:\n",
        "            self.use_proj=0\n",
        "        else:\n",
        "            self.use_proj=1\n",
        "        self.convresid=nn.utils.weight_norm(nn.Conv2d(in_c, out_c, kernel_size=(1,1)),name='weight',dim=0)\n",
        "        \n",
        "        self.leftpad = nn.ConstantPad2d((0,0,k-1,0),0)#(paddingLeft, paddingRight, paddingTop, paddingBottom)\n",
        "\n",
        "        if(useSDB):\n",
        "            #[bs,in_c,seq_length+(k-1)]->conv(1,in_c,in_c/downbot)->[bs,in_c/downbot,seq_length+(k-1)]\n",
        "            self.convx1a = nn.utils.weight_norm(nn.Conv2d(in_c*numberOfIndependentDendriticBranches, int(in_c/downbot)*numberOfIndependentDendriticBranches, kernel_size=(1,1), groups=numberOfIndependentDendriticBranches),name='weight',dim=0)\n",
        "            self.convx2a = nn.utils.weight_norm(nn.Conv2d(in_c*numberOfIndependentDendriticBranches, int(in_c/downbot)*numberOfIndependentDendriticBranches, kernel_size=(1,1), groups=numberOfIndependentDendriticBranches),name='weight',dim=0)\n",
        "            #[bs,in_c/downbot,seq_length+(k-1)]->conv(k,in_c/downbot,in_c/downbot)->[bs,in_c/downbot,seq_length]\n",
        "            self.convx1b = nn.utils.weight_norm(nn.Conv2d(int(in_c/downbot)*numberOfIndependentDendriticBranches, int(in_c/downbot)*numberOfIndependentDendriticBranches, kernel_size=(k,1), groups=numberOfIndependentDendriticBranches),name='weight',dim=0)\n",
        "            self.convx2b = nn.utils.weight_norm(nn.Conv2d(int(in_c/downbot)*numberOfIndependentDendriticBranches, int(in_c/downbot)*numberOfIndependentDendriticBranches, kernel_size=(k,1), groups=numberOfIndependentDendriticBranches),name='weight',dim=0)\n",
        "            #[bs,in_c/downbot,seq_length]->conv(1,in_c/downbot,out_c)->[bs,out_c,seq_length]\n",
        "            self.convx1c = nn.utils.weight_norm(nn.Conv2d(int(in_c/downbot)*numberOfIndependentDendriticBranches, out_c*numberOfIndependentDendriticBranches, kernel_size=(1,1), groups=numberOfIndependentDendriticBranches),name='weight',dim=0)\n",
        "            self.convx2c = nn.utils.weight_norm(nn.Conv2d(int(in_c/downbot)*numberOfIndependentDendriticBranches, out_c*numberOfIndependentDendriticBranches, kernel_size=(1,1), groups=numberOfIndependentDendriticBranches),name='weight',dim=0)\n",
        "        else:\n",
        "            #[bs,in_c,seq_length+(k-1)]->conv(1,in_c,in_c/downbot)->[bs,in_c/downbot,seq_length+(k-1)]\n",
        "            self.convx1a = nn.utils.weight_norm(nn.Conv2d(in_c, int(in_c/downbot), kernel_size=(1,1)),name='weight',dim=0)\n",
        "            self.convx2a = nn.utils.weight_norm(nn.Conv2d(in_c, int(in_c/downbot), kernel_size=(1,1)),name='weight',dim=0)\n",
        "            #[bs,in_c/downbot,seq_length+(k-1)]->conv(k,in_c/downbot,in_c/downbot)->[bs,in_c/downbot,seq_length]\n",
        "            self.convx1b = nn.utils.weight_norm(nn.Conv2d(int(in_c/downbot), int(in_c/downbot), kernel_size=(k,1)),name='weight',dim=0)\n",
        "            self.convx2b = nn.utils.weight_norm(nn.Conv2d(int(in_c/downbot), int(in_c/downbot), kernel_size=(k,1)),name='weight',dim=0)\n",
        "            #[bs,in_c/downbot,seq_length]->conv(1,in_c/downbot,out_c)->[bs,out_c,seq_length]\n",
        "            self.convx1c = nn.utils.weight_norm(nn.Conv2d(int(in_c/downbot), out_c, kernel_size=(1,1)),name='weight',dim=0)\n",
        "            self.convx2c = nn.utils.weight_norm(nn.Conv2d(int(in_c/downbot), out_c, kernel_size=(1,1)),name='weight',dim=0)\n",
        "\n",
        "    def calculateSDBconv(self, x, convObject, normObject=None):\n",
        "        #x = x.repeat(1, numberOfIndependentDendriticBranches, 1, 1)\n",
        "        x = torch.unsqueeze(x, dim=1)\n",
        "        x = x.expand(x.shape[0], numberOfIndependentDendriticBranches, x.shape[2], x.shape[3], x.shape[4])  #returns a new view of the tensor\n",
        "        x = torch.reshape(x, (x.shape[0], x.shape[1]*x.shape[2], x.shape[3], x.shape[4]))\n",
        "\n",
        "        x = convObject(x)\n",
        "        \n",
        "        x = torch.reshape(x, (x.shape[0], numberOfIndependentDendriticBranches, x.shape[1]//numberOfIndependentDendriticBranches, x.shape[2], x.shape[3]))\n",
        "        #x = torch.reshape(x, (x.shape[0], x.shape[2], x.shape[1], x.shape[3], x.shape[4]))  #optional (change order of tensors for max)\n",
        "        x = torch.permute(x, (0, 2, 1, 3, 4))\n",
        "        x = torch.max(x, dim=2, keepdim=False).values\n",
        "\n",
        "        if(normaliseActivationSparsity):\n",
        "            #x = x-weightStddev #reduce activation (since taking max value across independent segments will tend to be negative)\n",
        "            #x = normObject(x)\n",
        "            x = nn.functional.layer_norm(x, [x.shape[1], x.shape[2], x.shape[3]])\n",
        "\n",
        "        #print(\"x.shape = \", x.shape)\n",
        "        #print(\"x = \", x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        if self.use_proj==1:# if in_c != out_c, need to change size of residual\n",
        "            residual=self.convresid(residual)\n",
        "        x=self.leftpad(x) # [bs,in_c,seq_length+(k-1),1]\n",
        "        if(useSDB):\n",
        "            x1 = x\n",
        "            x1 = self.calculateSDBconv(x1, self.convx1a)\n",
        "            x1 = self.calculateSDBconv(x1, self.convx1b)\n",
        "            x1 = self.calculateSDBconv(x1, self.convx1c)\n",
        "            x2 = x\n",
        "            x2 = self.calculateSDBconv(x2, self.convx2a)\n",
        "            x2 = self.calculateSDBconv(x2, self.convx2b)\n",
        "            x2 = self.calculateSDBconv(x2, self.convx2c)\n",
        "        else:\n",
        "            x1 = self.convx1c(self.convx1b(self.convx1a(x))) # [bs,out_c,seq_length,1]\n",
        "            x2 = self.convx2c(self.convx2b(self.convx2a(x))) # [bs,out_c,seq_length,1]\n",
        "\n",
        "        x2 = torch.sigmoid(x2)\n",
        "        x=torch.mul(x1,x2) # [bs,out_c,seq_length,1]\n",
        "        return x+residual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "133FDcPuYWsZ"
      },
      "outputs": [],
      "source": [
        "class GCNNmodel(nn.Module):\n",
        "    def __init__(self, vs, emb_sz, k, nh, nl,downbot):\n",
        "    #def __init__(self, vs, emb_sz, k, nh, nl,dw,cutoffs):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embed = nn.Embedding(vs, emb_sz)\n",
        "        \n",
        "        self.inlayer=GLUblock(k,emb_sz,nh,downbot)\n",
        "        self.GLUlayers=self.make_GLU_layers(k,nh,nl,downbot)\n",
        "        self.out=nn.AdaptiveLogSoftmaxWithLoss(nh, vs, cutoffs=[round(vs/25),round(vs/5)],div_value=4)\n",
        "\n",
        "    def make_GLU_layers(self, k, nh, nl, downbot):\n",
        "        layers = [GLUblock(k, nh, nh, downbot) for i in range(nl)]\n",
        "        return nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        target=x[1:,:]\n",
        "        target=target.contiguous().view(target.size()[0]*target.size()[1])#[seq_length*bs,out_c]\n",
        "        x=x[:-1,:]\n",
        "        \n",
        "        #first block\n",
        "        x = self.embed(torch.t(x)) # x -> [seq_length,bs] -> [bs,seq_length] -> [bs,seq_length,emb_sz] ... i.e. transpose 1st\n",
        "        x=torch.transpose(x, 1, 2) #[bs,emb_sz,seq_length]    \n",
        "        x = x.unsqueeze(3)  # [bs,emb_sz,seq_length,1]\n",
        "        x=self.inlayer(x) #[bs,nh,seq_length,1]\n",
        "             \n",
        "        #residual GLU blocks\n",
        "        x=self.GLUlayers(x) # [bs,nh,seq_length,1]\n",
        "        \n",
        "        #out\n",
        "        x=torch.squeeze(x,3) #[bs,out_c,seq_length]\n",
        "        x=torch.transpose(x, 1, 2) #[bs,seq_length,out_c]\n",
        "        x=torch.transpose(x, 0, 1) #[seq_length,bs,out_c]\n",
        "        x=x.contiguous().view(-1,x.size()[2])#[seq_length*bs,out_c]\n",
        "        outta=self.out(x,target)\n",
        "        \n",
        "        return    outta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrGvpYy1YWsa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a573b3d3-a73e-40f3-eb89-5087b70eaaec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCNNmodel(\n",
            "  (embed): Embedding(28112, 300)\n",
            "  (inlayer): GLUblock(\n",
            "    (convresid): Conv2d(300, 600, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (leftpad): ConstantPad2d(padding=(0, 0, 3, 0), value=0)\n",
            "    (convx1a): Conv2d(3000, 150, kernel_size=(1, 1), stride=(1, 1), groups=10)\n",
            "    (convx2a): Conv2d(3000, 150, kernel_size=(1, 1), stride=(1, 1), groups=10)\n",
            "    (convx1b): Conv2d(150, 150, kernel_size=(4, 1), stride=(1, 1), groups=10)\n",
            "    (convx2b): Conv2d(150, 150, kernel_size=(4, 1), stride=(1, 1), groups=10)\n",
            "    (convx1c): Conv2d(150, 6000, kernel_size=(1, 1), stride=(1, 1), groups=10)\n",
            "    (convx2c): Conv2d(150, 6000, kernel_size=(1, 1), stride=(1, 1), groups=10)\n",
            "  )\n",
            "  (GLUlayers): Sequential(\n",
            "    (0): GLUblock(\n",
            "      (convresid): Conv2d(600, 600, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (leftpad): ConstantPad2d(padding=(0, 0, 3, 0), value=0)\n",
            "      (convx1a): Conv2d(6000, 300, kernel_size=(1, 1), stride=(1, 1), groups=10)\n",
            "      (convx2a): Conv2d(6000, 300, kernel_size=(1, 1), stride=(1, 1), groups=10)\n",
            "      (convx1b): Conv2d(300, 300, kernel_size=(4, 1), stride=(1, 1), groups=10)\n",
            "      (convx2b): Conv2d(300, 300, kernel_size=(4, 1), stride=(1, 1), groups=10)\n",
            "      (convx1c): Conv2d(300, 6000, kernel_size=(1, 1), stride=(1, 1), groups=10)\n",
            "      (convx2c): Conv2d(300, 6000, kernel_size=(1, 1), stride=(1, 1), groups=10)\n",
            "    )\n",
            "    (1): GLUblock(\n",
            "      (convresid): Conv2d(600, 600, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (leftpad): ConstantPad2d(padding=(0, 0, 3, 0), value=0)\n",
            "      (convx1a): Conv2d(6000, 300, kernel_size=(1, 1), stride=(1, 1), groups=10)\n",
            "      (convx2a): Conv2d(6000, 300, kernel_size=(1, 1), stride=(1, 1), groups=10)\n",
            "      (convx1b): Conv2d(300, 300, kernel_size=(4, 1), stride=(1, 1), groups=10)\n",
            "      (convx2b): Conv2d(300, 300, kernel_size=(4, 1), stride=(1, 1), groups=10)\n",
            "      (convx1c): Conv2d(300, 6000, kernel_size=(1, 1), stride=(1, 1), groups=10)\n",
            "      (convx2c): Conv2d(300, 6000, kernel_size=(1, 1), stride=(1, 1), groups=10)\n",
            "    )\n",
            "    (2): GLUblock(\n",
            "      (convresid): Conv2d(600, 600, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (leftpad): ConstantPad2d(padding=(0, 0, 3, 0), value=0)\n",
            "      (convx1a): Conv2d(6000, 300, kernel_size=(1, 1), stride=(1, 1), groups=10)\n",
            "      (convx2a): Conv2d(6000, 300, kernel_size=(1, 1), stride=(1, 1), groups=10)\n",
            "      (convx1b): Conv2d(300, 300, kernel_size=(4, 1), stride=(1, 1), groups=10)\n",
            "      (convx2b): Conv2d(300, 300, kernel_size=(4, 1), stride=(1, 1), groups=10)\n",
            "      (convx1c): Conv2d(300, 6000, kernel_size=(1, 1), stride=(1, 1), groups=10)\n",
            "      (convx2c): Conv2d(300, 6000, kernel_size=(1, 1), stride=(1, 1), groups=10)\n",
            "    )\n",
            "    (3): GLUblock(\n",
            "      (convresid): Conv2d(600, 600, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (leftpad): ConstantPad2d(padding=(0, 0, 3, 0), value=0)\n",
            "      (convx1a): Conv2d(6000, 300, kernel_size=(1, 1), stride=(1, 1), groups=10)\n",
            "      (convx2a): Conv2d(6000, 300, kernel_size=(1, 1), stride=(1, 1), groups=10)\n",
            "      (convx1b): Conv2d(300, 300, kernel_size=(4, 1), stride=(1, 1), groups=10)\n",
            "      (convx2b): Conv2d(300, 300, kernel_size=(4, 1), stride=(1, 1), groups=10)\n",
            "      (convx1c): Conv2d(300, 6000, kernel_size=(1, 1), stride=(1, 1), groups=10)\n",
            "      (convx2c): Conv2d(300, 6000, kernel_size=(1, 1), stride=(1, 1), groups=10)\n",
            "    )\n",
            "  )\n",
            "  (out): AdaptiveLogSoftmaxWithLoss(\n",
            "    (head): Linear(in_features=600, out_features=1126, bias=False)\n",
            "    (tail): ModuleList(\n",
            "      (0): Sequential(\n",
            "        (0): Linear(in_features=600, out_features=150, bias=False)\n",
            "        (1): Linear(in_features=150, out_features=4498, bias=False)\n",
            "      )\n",
            "      (1): Sequential(\n",
            "        (0): Linear(in_features=600, out_features=37, bias=False)\n",
            "        (1): Linear(in_features=37, out_features=22490, bias=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "#create GCNN \n",
        "GCNNnet=modeler(train_loader,val_loader, GCNNmodel(vs, emb_sz, k, nh, nl, downbot),modelvals)\n",
        "print(GCNNnet.model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVQbks3YYWsc"
      },
      "outputs": [],
      "source": [
        "#load the glove-vectors into the model\n",
        "new_w=np.load(DATAPATH/'emb_wgts300_proc_par2.npy') #load embedding weights\n",
        "GCNNnet.model.embed.weight.data=torch.FloatTensor(new_w).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpmiGeF4YWsd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5b48fd6-c820-477e-d1a4-1ca3963bb590"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-c3f0473d6ce3>:51: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  sort_idx = np.concatenate(np.random.permutation(ck_idx[1:]))\n",
            "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0\tTotal_its: 0.00M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 10.0069\tTime: 0.16\tLR: 1.0000\n",
            "Train Epoch: 0\tTotal_its: 0.01M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 6.4654\tTime: 1.43\tLR: 1.0000\n",
            "Train Epoch: 0\tTotal_its: 0.01M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 5.9440\tTime: 2.67\tLR: 1.0000\n",
            "Validation Loss: 5.8186\tPerp: 336.4896\n",
            "Train Epoch: 1\tTotal_its: 0.01M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 5.0492\tTime: 3.80\tLR: 1.0000\n",
            "Train Epoch: 1\tTotal_its: 0.02M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 5.9143\tTime: 5.05\tLR: 1.0000\n",
            "Train Epoch: 1\tTotal_its: 0.02M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 4.3437\tTime: 6.77\tLR: 1.0000\n",
            "Validation Loss: 5.3693\tPerp: 214.7115\n",
            "Train Epoch: 2\tTotal_its: 0.03M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.8336\tTime: 8.10\tLR: 1.0000\n",
            "Train Epoch: 2\tTotal_its: 0.03M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 3.2961\tTime: 9.55\tLR: 1.0000\n",
            "Train Epoch: 2\tTotal_its: 0.04M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 5.3445\tTime: 11.26\tLR: 1.0000\n",
            "Validation Loss: 5.2472\tPerp: 190.0342\n",
            "Train Epoch: 3\tTotal_its: 0.04M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.7916\tTime: 12.59\tLR: 1.0000\n",
            "Train Epoch: 3\tTotal_its: 0.05M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 4.4660\tTime: 14.08\tLR: 1.0000\n",
            "Train Epoch: 3\tTotal_its: 0.05M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 3.9424\tTime: 15.76\tLR: 1.0000\n",
            "Validation Loss: 5.1928\tPerp: 179.9718\n",
            "Train Epoch: 4\tTotal_its: 0.06M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.6657\tTime: 17.21\tLR: 1.0000\n",
            "Train Epoch: 4\tTotal_its: 0.06M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 3.7520\tTime: 18.48\tLR: 1.0000\n",
            "Train Epoch: 4\tTotal_its: 0.07M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 4.4165\tTime: 19.90\tLR: 1.0000\n",
            "Validation Loss: 5.1217\tPerp: 167.6265\n",
            "Train Epoch: 5\tTotal_its: 0.07M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.6446\tTime: 21.36\tLR: 1.0000\n",
            "Train Epoch: 5\tTotal_its: 0.07M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 5.3468\tTime: 22.53\tLR: 1.0000\n",
            "Train Epoch: 5\tTotal_its: 0.08M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 5.1551\tTime: 23.84\tLR: 1.0000\n",
            "Validation Loss: 5.0042\tPerp: 149.0324\n",
            "Train Epoch: 6\tTotal_its: 0.08M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.4967\tTime: 25.28\tLR: 1.0000\n",
            "Train Epoch: 6\tTotal_its: 0.09M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 5.1799\tTime: 26.80\tLR: 1.0000\n",
            "Train Epoch: 6\tTotal_its: 0.09M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 4.9476\tTime: 28.53\tLR: 1.0000\n",
            "Validation Loss: 4.8772\tPerp: 131.2561\n",
            "Train Epoch: 7\tTotal_its: 0.10M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.4887\tTime: 29.90\tLR: 1.0000\n",
            "Train Epoch: 7\tTotal_its: 0.10M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 5.1138\tTime: 31.12\tLR: 1.0000\n",
            "Train Epoch: 7\tTotal_its: 0.11M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 5.3089\tTime: 32.36\tLR: 1.0000\n",
            "Validation Loss: 4.8241\tPerp: 124.4773\n",
            "Train Epoch: 8\tTotal_its: 0.11M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.4033\tTime: 33.79\tLR: 1.0000\n",
            "Train Epoch: 8\tTotal_its: 0.12M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 3.4542\tTime: 35.21\tLR: 1.0000\n",
            "Train Epoch: 8\tTotal_its: 0.12M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 2.4763\tTime: 36.92\tLR: 1.0000\n",
            "Validation Loss: 4.8147\tPerp: 123.3052\n",
            "Train Epoch: 9\tTotal_its: 0.13M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.2805\tTime: 38.36\tLR: 1.0000\n",
            "Train Epoch: 9\tTotal_its: 0.13M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 5.1247\tTime: 39.61\tLR: 1.0000\n",
            "Train Epoch: 9\tTotal_its: 0.14M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 5.1612\tTime: 40.88\tLR: 1.0000\n",
            "Validation Loss: 4.7656\tPerp: 117.3965\n",
            "Train Epoch: 10\tTotal_its: 0.14M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.3465\tTime: 42.27\tLR: 1.0000\n",
            "Train Epoch: 10\tTotal_its: 0.14M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 4.5083\tTime: 43.47\tLR: 1.0000\n",
            "Train Epoch: 10\tTotal_its: 0.15M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 2.6441\tTime: 44.90\tLR: 1.0000\n",
            "Validation Loss: 4.6847\tPerp: 108.2770\n",
            "Train Epoch: 11\tTotal_its: 0.15M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.3004\tTime: 46.28\tLR: 1.0000\n",
            "Train Epoch: 11\tTotal_its: 0.16M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 4.8824\tTime: 47.60\tLR: 1.0000\n",
            "Train Epoch: 11\tTotal_its: 0.16M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 5.1016\tTime: 48.88\tLR: 1.0000\n",
            "Validation Loss: 4.6332\tPerp: 102.8412\n",
            "Train Epoch: 12\tTotal_its: 0.17M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.1585\tTime: 50.05\tLR: 1.0000\n",
            "Train Epoch: 12\tTotal_its: 0.17M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 3.5748\tTime: 51.26\tLR: 1.0000\n",
            "Train Epoch: 12\tTotal_its: 0.18M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 3.8566\tTime: 53.06\tLR: 1.0000\n",
            "Validation Loss: 4.6301\tPerp: 102.5200\n",
            "Train Epoch: 13\tTotal_its: 0.18M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.0995\tTime: 54.43\tLR: 1.0000\n",
            "Train Epoch: 13\tTotal_its: 0.19M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 3.7232\tTime: 55.68\tLR: 1.0000\n",
            "Train Epoch: 13\tTotal_its: 0.19M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 4.3785\tTime: 57.29\tLR: 1.0000\n",
            "Validation Loss: 4.5950\tPerp: 98.9848\n",
            "Train Epoch: 14\tTotal_its: 0.20M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.0904\tTime: 58.80\tLR: 1.0000\n",
            "Train Epoch: 14\tTotal_its: 0.20M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 2.6673\tTime: 60.21\tLR: 1.0000\n",
            "Train Epoch: 14\tTotal_its: 0.21M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 3.0622\tTime: 62.02\tLR: 1.0000\n",
            "Validation Loss: 4.5858\tPerp: 98.0863\n",
            "Train Epoch: 15\tTotal_its: 0.21M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.0714\tTime: 63.41\tLR: 1.0000\n",
            "Train Epoch: 15\tTotal_its: 0.21M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 2.5768\tTime: 64.69\tLR: 1.0000\n",
            "Train Epoch: 15\tTotal_its: 0.22M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 3.6688\tTime: 66.39\tLR: 1.0000\n",
            "Validation Loss: 4.4953\tPerp: 89.5977\n",
            "Train Epoch: 16\tTotal_its: 0.22M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.1138\tTime: 67.93\tLR: 1.0000\n",
            "Train Epoch: 16\tTotal_its: 0.23M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 5.4921\tTime: 69.15\tLR: 1.0000\n",
            "Train Epoch: 16\tTotal_its: 0.23M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 3.6453\tTime: 70.64\tLR: 1.0000\n",
            "Validation Loss: 4.6859\tPerp: 108.4041\n",
            "Train Epoch: 17\tTotal_its: 0.24M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.1532\tTime: 72.15\tLR: 1.0000\n",
            "Train Epoch: 17\tTotal_its: 0.24M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 4.1064\tTime: 73.67\tLR: 1.0000\n",
            "Train Epoch: 17\tTotal_its: 0.25M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 4.4478\tTime: 75.38\tLR: 1.0000\n",
            "Validation Loss: 4.7775\tPerp: 118.8054\n",
            "Train Epoch: 18\tTotal_its: 0.25M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.1749\tTime: 76.78\tLR: 1.0000\n",
            "Train Epoch: 18\tTotal_its: 0.26M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 4.0462\tTime: 78.29\tLR: 1.0000\n",
            "Train Epoch: 18\tTotal_its: 0.26M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 4.2294\tTime: 79.92\tLR: 1.0000\n",
            "Validation Loss: 4.7301\tPerp: 113.3081\n",
            "Train Epoch: 19\tTotal_its: 0.26M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.0149\tTime: 81.35\tLR: 1.0000\n",
            "Train Epoch: 19\tTotal_its: 0.27M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 4.8955\tTime: 82.67\tLR: 1.0000\n",
            "Train Epoch: 19\tTotal_its: 0.27M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 2.7154\tTime: 84.28\tLR: 1.0000\n",
            "Validation Loss: 4.4139\tPerp: 82.5901\n",
            "Train Epoch: 20\tTotal_its: 0.28M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 3.9132\tTime: 85.65\tLR: 1.0000\n",
            "Train Epoch: 20\tTotal_its: 0.28M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 4.6754\tTime: 86.92\tLR: 1.0000\n",
            "Train Epoch: 20\tTotal_its: 0.29M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 4.5387\tTime: 88.18\tLR: 1.0000\n",
            "Validation Loss: 4.4631\tPerp: 86.7539\n",
            "Train Epoch: 21\tTotal_its: 0.29M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 3.9655\tTime: 89.56\tLR: 1.0000\n",
            "Train Epoch: 21\tTotal_its: 0.30M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 4.5578\tTime: 90.92\tLR: 1.0000\n",
            "Train Epoch: 21\tTotal_its: 0.30M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 4.6758\tTime: 92.28\tLR: 1.0000\n",
            "Validation Loss: 4.3151\tPerp: 74.8186\n",
            "Train Epoch: 22\tTotal_its: 0.31M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 3.9233\tTime: 93.35\tLR: 1.0000\n",
            "Train Epoch: 22\tTotal_its: 0.31M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 4.0739\tTime: 94.99\tLR: 1.0000\n",
            "Train Epoch: 22\tTotal_its: 0.32M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 4.3152\tTime: 96.74\tLR: 1.0000\n",
            "Validation Loss: 4.3158\tPerp: 74.8770\n",
            "Train Epoch: 23\tTotal_its: 0.32M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 3.9149\tTime: 98.03\tLR: 1.0000\n",
            "Train Epoch: 23\tTotal_its: 0.33M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 4.5914\tTime: 99.33\tLR: 1.0000\n",
            "Train Epoch: 23\tTotal_its: 0.33M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 2.2326\tTime: 100.98\tLR: 1.0000\n",
            "Validation Loss: 4.4276\tPerp: 83.7271\n",
            "Train Epoch: 24\tTotal_its: 0.33M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 3.9599\tTime: 102.44\tLR: 1.0000\n",
            "Train Epoch: 24\tTotal_its: 0.34M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 4.2572\tTime: 103.70\tLR: 1.0000\n",
            "Train Epoch: 24\tTotal_its: 0.34M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 4.4139\tTime: 105.03\tLR: 1.0000\n",
            "Validation Loss: 4.2813\tPerp: 72.3337\n",
            "Train Epoch: 25\tTotal_its: 0.35M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 3.8891\tTime: 106.12\tLR: 1.0000\n",
            "Train Epoch: 25\tTotal_its: 0.35M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 4.2384\tTime: 107.50\tLR: 1.0000\n",
            "Train Epoch: 25\tTotal_its: 0.36M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 2.0902\tTime: 109.19\tLR: 1.0000\n",
            "Validation Loss: 4.3165\tPerp: 74.9232\n",
            "Train Epoch: 26\tTotal_its: 0.36M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 3.8758\tTime: 110.68\tLR: 1.0000\n",
            "Train Epoch: 26\tTotal_its: 0.37M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 4.3310\tTime: 111.87\tLR: 1.0000\n",
            "Train Epoch: 26\tTotal_its: 0.37M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 4.4355\tTime: 113.25\tLR: 1.0000\n",
            "Validation Loss: 4.3673\tPerp: 78.8342\n",
            "Train Epoch: 27\tTotal_its: 0.38M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 3.8105\tTime: 114.40\tLR: 1.0000\n",
            "Train Epoch: 27\tTotal_its: 0.38M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 4.3820\tTime: 115.70\tLR: 1.0000\n",
            "Train Epoch: 27\tTotal_its: 0.39M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 4.2949\tTime: 117.01\tLR: 1.0000\n",
            "Validation Loss: 4.2278\tPerp: 68.5687\n",
            "Train Epoch: 28\tTotal_its: 0.39M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 3.7084\tTime: 118.21\tLR: 1.0000\n",
            "Train Epoch: 28\tTotal_its: 0.40M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 4.4209\tTime: 119.52\tLR: 1.0000\n",
            "Train Epoch: 28\tTotal_its: 0.40M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 2.2595\tTime: 120.77\tLR: 1.0000\n",
            "Validation Loss: 4.2077\tPerp: 67.2032\n",
            "Train Epoch: 29\tTotal_its: 0.40M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 3.8338\tTime: 122.19\tLR: 1.0000\n",
            "Train Epoch: 29\tTotal_its: 0.41M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 2.4217\tTime: 123.64\tLR: 1.0000\n",
            "Train Epoch: 29\tTotal_its: 0.41M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 2.8592\tTime: 125.29\tLR: 1.0000\n",
            "Validation Loss: 4.2301\tPerp: 68.7251\n",
            "Train Epoch: 30\tTotal_its: 0.42M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 3.7730\tTime: 126.78\tLR: 1.0000\n",
            "Train Epoch: 30\tTotal_its: 0.42M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 3.4946\tTime: 128.18\tLR: 1.0000\n",
            "Train Epoch: 30\tTotal_its: 0.43M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 2.6861\tTime: 129.77\tLR: 1.0000\n",
            "Validation Loss: 4.2410\tPerp: 69.4768\n",
            "Train Epoch: 31\tTotal_its: 0.43M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 3.8620\tTime: 131.26\tLR: 1.0000\n",
            "Train Epoch: 31\tTotal_its: 0.44M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 4.3874\tTime: 132.53\tLR: 1.0000\n",
            "Train Epoch: 31\tTotal_its: 0.44M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 3.5943\tTime: 133.86\tLR: 1.0000\n",
            "Validation Loss: 4.2316\tPerp: 68.8299\n",
            "Train Epoch: 32\tTotal_its: 0.45M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 3.9085\tTime: 135.29\tLR: 1.0000\n",
            "Train Epoch: 32\tTotal_its: 0.45M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 4.2858\tTime: 136.37\tLR: 1.0000\n",
            "Train Epoch: 32\tTotal_its: 0.46M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 2.4604\tTime: 138.00\tLR: 1.0000\n",
            "Validation Loss: 4.2401\tPerp: 69.4161\n",
            "Train Epoch: 33\tTotal_its: 0.46M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 3.6552\tTime: 139.52\tLR: 1.0000\n",
            "Train Epoch: 33\tTotal_its: 0.46M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 3.4958\tTime: 140.93\tLR: 1.0000\n",
            "Train Epoch: 33\tTotal_its: 0.47M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 2.0818\tTime: 142.63\tLR: 1.0000\n",
            "Validation Loss: 4.2149\tPerp: 67.6904\n",
            "Train Epoch: 34\tTotal_its: 0.47M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 3.6955\tTime: 144.02\tLR: 1.0000\n",
            "Train Epoch: 34\tTotal_its: 0.48M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 2.2001\tTime: 145.24\tLR: 1.0000\n",
            "Train Epoch: 34\tTotal_its: 0.48M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 2.8553\tTime: 146.67\tLR: 1.0000\n",
            "Validation Loss: 4.5116\tPerp: 91.0654\n",
            "Train Epoch: 35\tTotal_its: 0.49M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 3.9983\tTime: 148.11\tLR: 1.0000\n",
            "Train Epoch: 35\tTotal_its: 0.49M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 3.3093\tTime: 149.86\tLR: 1.0000\n",
            "Train Epoch: 35\tTotal_its: 0.50M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 3.4294\tTime: 151.47\tLR: 1.0000\n",
            "Validation Loss: 4.2348\tPerp: 69.0501\n",
            "Train Epoch: 36\tTotal_its: 0.50M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 3.7666\tTime: 152.86\tLR: 1.0000\n",
            "Train Epoch: 36\tTotal_its: 0.51M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 3.0481\tTime: 154.28\tLR: 1.0000\n",
            "Train Epoch: 36\tTotal_its: 0.51M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 2.6393\tTime: 155.86\tLR: 1.0000\n",
            "Validation Loss: 4.2074\tPerp: 67.1784\n",
            "Train Epoch: 37\tTotal_its: 0.52M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 3.7265\tTime: 157.35\tLR: 1.0000\n",
            "Train Epoch: 37\tTotal_its: 0.52M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 4.7400\tTime: 158.62\tLR: 1.0000\n",
            "Train Epoch: 37\tTotal_its: 0.53M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 4.7044\tTime: 159.88\tLR: 1.0000\n",
            "Validation Loss: 4.3870\tPerp: 80.4006\n",
            "Train Epoch: 38\tTotal_its: 0.53M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 3.8157\tTime: 161.30\tLR: 1.0000\n",
            "Train Epoch: 38\tTotal_its: 0.53M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 4.3051\tTime: 162.57\tLR: 1.0000\n",
            "Train Epoch: 38\tTotal_its: 0.54M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 4.3040\tTime: 163.87\tLR: 1.0000\n",
            "Validation Loss: 4.1568\tPerp: 63.8654\n",
            "Train Epoch: 39\tTotal_its: 0.54M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 3.6119\tTime: 164.97\tLR: 1.0000\n",
            "Train Epoch: 39\tTotal_its: 0.55M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 2.2832\tTime: 166.52\tLR: 1.0000\n",
            "Train Epoch: 39\tTotal_its: 0.55M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 2.0857\tTime: 168.20\tLR: 1.0000\n",
            "Validation Loss: 4.2684\tPerp: 71.4103\n",
            "Train Epoch: 40\tTotal_its: 0.56M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 3.7976\tTime: 169.59\tLR: 1.0000\n",
            "Train Epoch: 40\tTotal_its: 0.56M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 4.0699\tTime: 170.93\tLR: 1.0000\n",
            "Train Epoch: 40\tTotal_its: 0.57M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 4.3166\tTime: 172.14\tLR: 1.0000\n",
            "Validation Loss: 4.1152\tPerp: 61.2652\n",
            "Train Epoch: 41\tTotal_its: 0.57M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 3.7429\tTime: 173.50\tLR: 1.0000\n",
            "Train Epoch: 41\tTotal_its: 0.58M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 1.1819\tTime: 174.78\tLR: 1.0000\n",
            "Train Epoch: 41\tTotal_its: 0.58M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 4.2494\tTime: 176.05\tLR: 1.0000\n",
            "Validation Loss: 4.1376\tPerp: 62.6543\n",
            "Train Epoch: 42\tTotal_its: 0.58M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 3.5870\tTime: 177.32\tLR: 1.0000\n",
            "Train Epoch: 42\tTotal_its: 0.59M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 1.1208\tTime: 178.64\tLR: 1.0000\n",
            "Train Epoch: 42\tTotal_its: 0.59M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 4.3393\tTime: 179.84\tLR: 1.0000\n",
            "Validation Loss: 4.1985\tPerp: 66.5890\n",
            "Train Epoch: 43\tTotal_its: 0.60M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 3.7240\tTime: 181.03\tLR: 1.0000\n",
            "Train Epoch: 43\tTotal_its: 0.60M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 4.9195\tTime: 182.41\tLR: 1.0000\n",
            "Train Epoch: 43\tTotal_its: 0.61M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 1.4012\tTime: 183.59\tLR: 1.0000\n",
            "Validation Loss: 4.1292\tPerp: 62.1276\n",
            "Train Epoch: 44\tTotal_its: 0.61M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 3.6544\tTime: 184.88\tLR: 1.0000\n",
            "Train Epoch: 44\tTotal_its: 0.62M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 4.0423\tTime: 186.15\tLR: 1.0000\n",
            "Train Epoch: 44\tTotal_its: 0.62M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 2.1698\tTime: 187.46\tLR: 1.0000\n",
            "Validation Loss: 4.2667\tPerp: 71.2870\n",
            "Train Epoch: 45\tTotal_its: 0.63M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 3.6765\tTime: 188.84\tLR: 1.0000\n",
            "Train Epoch: 45\tTotal_its: 0.63M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 3.1044\tTime: 190.28\tLR: 1.0000\n",
            "Train Epoch: 45\tTotal_its: 0.64M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 2.2284\tTime: 192.00\tLR: 1.0000\n",
            "Validation Loss: 4.0704\tPerp: 58.5805\n",
            "Train Epoch: 46\tTotal_its: 0.64M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 3.5998\tTime: 193.37\tLR: 1.0000\n",
            "Train Epoch: 46\tTotal_its: 0.65M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 3.0571\tTime: 194.92\tLR: 1.0000\n",
            "Train Epoch: 46\tTotal_its: 0.65M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 3.4296\tTime: 196.50\tLR: 1.0000\n",
            "Validation Loss: 4.1530\tPerp: 63.6216\n",
            "Train Epoch: 47\tTotal_its: 0.65M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 3.6589\tTime: 197.84\tLR: 1.0000\n",
            "Train Epoch: 47\tTotal_its: 0.66M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 3.6190\tTime: 199.13\tLR: 1.0000\n",
            "Train Epoch: 47\tTotal_its: 0.66M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 6.7538\tTime: 200.23\tLR: 1.0000\n",
            "Validation Loss: 5.0320\tPerp: 153.2316\n",
            "Train Epoch: 48\tTotal_its: 0.67M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.3050\tTime: 201.50\tLR: 1.0000\n",
            "Train Epoch: 48\tTotal_its: 0.67M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 2.5056\tTime: 202.76\tLR: 1.0000\n",
            "Train Epoch: 48\tTotal_its: 0.68M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 4.1339\tTime: 204.01\tLR: 1.0000\n",
            "Validation Loss: 4.2742\tPerp: 71.8256\n",
            "Train Epoch: 49\tTotal_its: 0.68M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 3.8295\tTime: 205.50\tLR: 1.0000\n",
            "Train Epoch: 49\tTotal_its: 0.69M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 4.0388\tTime: 206.76\tLR: 1.0000\n",
            "Train Epoch: 49\tTotal_its: 0.69M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 3.9861\tTime: 208.06\tLR: 1.0000\n",
            "Validation Loss: 4.1001\tPerp: 60.3479\n",
            "The end! 209.31 minutes\n"
          ]
        }
      ],
      "source": [
        "GCNNnet.model_fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdptQNjzYWsd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEzaJs6_YWse"
      },
      "outputs": [],
      "source": [
        "epoch_list,loss_list,perp_list,time_list=zip(*GCNNnet.modelvals['val_loss_list'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3T8IX75QYWsf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "outputId": "b13cfe1f-be8d-435d-e7ab-c4d0ac62463a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(list(range(3,51)),perp_list[2:])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Perplexity')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3LJBlSjYWsi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}